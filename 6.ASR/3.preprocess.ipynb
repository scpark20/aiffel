{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39354ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isdir, isfile, join\n",
    "\n",
    "from kspon_jamo import text_to_tokens, tokens_to_text, n_symbols, normalize_ksponspeech, SOS, EOS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7d68f6",
   "metadata": {},
   "source": [
    "### Data file 리스트 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "788ffa43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of data files : 1000\n",
      "{'wav': 'dataset/KsponSpeech_000001.wav', 'txt': 'dataset/KsponSpeech_000001.txt'}\n",
      "{'wav': 'dataset/KsponSpeech_000002.wav', 'txt': 'dataset/KsponSpeech_000002.txt'}\n",
      "{'wav': 'dataset/KsponSpeech_000003.wav', 'txt': 'dataset/KsponSpeech_000003.txt'}\n",
      "{'wav': 'dataset/KsponSpeech_000004.wav', 'txt': 'dataset/KsponSpeech_000004.txt'}\n",
      "{'wav': 'dataset/KsponSpeech_000005.wav', 'txt': 'dataset/KsponSpeech_000005.txt'}\n",
      "{'wav': 'dataset/KsponSpeech_000006.wav', 'txt': 'dataset/KsponSpeech_000006.txt'}\n",
      "{'wav': 'dataset/KsponSpeech_000007.wav', 'txt': 'dataset/KsponSpeech_000007.txt'}\n",
      "{'wav': 'dataset/KsponSpeech_000008.wav', 'txt': 'dataset/KsponSpeech_000008.txt'}\n",
      "{'wav': 'dataset/KsponSpeech_000009.wav', 'txt': 'dataset/KsponSpeech_000009.txt'}\n",
      "{'wav': 'dataset/KsponSpeech_000010.wav', 'txt': 'dataset/KsponSpeech_000010.txt'}\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'dataset'\n",
    "data_files = [{'wav': join(data_dir, file),\\\n",
    "               'txt': join(data_dir, file[:-3] + 'txt')} for file in listdir(data_dir) if '.wav' in file]\n",
    "print('# of data files :', len(data_files))\n",
    "data_files.sort(key=lambda x:x['txt'])\n",
    "for data_file in data_files[:10]:\n",
    "    print(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4d9e08",
   "metadata": {},
   "source": [
    "### txt 파일 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0361539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b/ n/ 그래서 지호랑 계단 n/ 올라와서 b/ 막 위에 운동하는 기구 있대요. b/ 그서 그걸로 운동 할려구요. b/ n/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file = data_files[2]['txt']\n",
    "with open(file, 'r', encoding='cp949') as f:\n",
    "    l = f.read()\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e853832",
   "metadata": {},
   "source": [
    "### 텍스트 normalization하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a103067a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "그래서 지호랑 계단 올라와서 막 위에 운동하는 기구 있대요. 그서 그걸로 운동 할려구요.\n"
     ]
    }
   ],
   "source": [
    "l = normalize_ksponspeech(l)\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed575411",
   "metadata": {},
   "source": [
    "### token으로 바꾸기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "595c1305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[257 295 256 260 297 256 263 289 256  32 265 296 256 270 291 256 260 287\n",
      " 264 256  32 257 300 256 259 287 258 256  32 264 291 260 256 260 287 256\n",
      " 264 301 256 263 289 256  32 261 287 257 256  32 264 306 256 264 299 256\n",
      "  32 264 293 258 256 259 291 264 256 270 287 256 258 295 258 256  32 257\n",
      " 296 256 257 293 256  32 264 296 274 256 259 297 256 264 292 256  46  32\n",
      " 257 295 256 263 289 256  32 257 295 256 257 289 260 256 260 291 256  32\n",
      " 264 293 258 256 259 291 264 256  32 270 287 260 256 260 290 256 257 293\n",
      " 256 264 292 256  46]\n"
     ]
    }
   ],
   "source": [
    "tokens = text_to_tokens(l)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc29bcf",
   "metadata": {},
   "source": [
    "### token을 텍스트로 바꾸기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce663350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "그래서 지호랑 계단 올라와서 막 위에 운동하는 기구 있대요. 그서 그걸로 운동 할려구요.\n"
     ]
    }
   ],
   "source": [
    "text_recon = tokens_to_text(tokens)\n",
    "print(text_recon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ee9bb4",
   "metadata": {},
   "source": [
    "### SOS, EOS 추가하기 (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1ce0b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[308 257 295 256 260 297 256 263 289 256  32 265 296 256 270 291 256 260\n",
      " 287 264 256  32 257 300 256 259 287 258 256  32 264 291 260 256 260 287\n",
      " 256 264 301 256 263 289 256  32 261 287 257 256  32 264 306 256 264 299\n",
      " 256  32 264 293 258 256 259 291 264 256 270 287 256 258 295 258 256  32\n",
      " 257 296 256 257 293 256  32 264 296 274 256 259 297 256 264 292 256  46\n",
      "  32 257 295 256 263 289 256  32 257 295 256 257 289 260 256 260 291 256\n",
      "  32 264 293 258 256 259 291 264 256  32 270 287 260 256 260 290 256 257\n",
      " 293 256 264 292 256  46 309]\n"
     ]
    }
   ],
   "source": [
    "tokens = np.concatenate([[SOS], tokens, [EOS]])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e3834c",
   "metadata": {},
   "source": [
    "### Pytorch Dataset Class 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c3048e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d49dc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KSponSpeechDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_files = [{'wav': join(data_dir, file),\\\n",
    "                            'txt': join(data_dir, file[:-3] + 'txt')} for file in listdir(data_dir) if '.wav' in file]\n",
    "\n",
    "    def _get_audio(self, file):\n",
    "        # (time,)\n",
    "        wav, _ = librosa.core.load(file, sr=16000, mono=True)\n",
    "        # (512, time)\n",
    "        S = librosa.feature.melspectrogram(wav, sr=16000, n_fft=1024, n_mels=80, hop_length=256, power=1.0)\n",
    "        S = np.log10(S + 1e-5) \n",
    "        # (time, 512)\n",
    "        return S.T\n",
    "            \n",
    "    def _get_text(self, file):\n",
    "        with open(file, 'r', encoding='cp949') as f:\n",
    "            l = f.read()\n",
    "            l = normalize_ksponspeech(l)\n",
    "            array = text_to_tokens(l)\n",
    "        # Insert SOS and EOS\n",
    "        array = np.concatenate([[SOS], array, [EOS]])\n",
    "        return array\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        while True:\n",
    "            text = self._get_text(self.data_files[index]['txt'])\n",
    "            if len(text) > 180:\n",
    "                index = (index + 1) % self.__len__()\n",
    "                continue\n",
    "\n",
    "            audio = self._get_audio(self.data_files[index]['wav'])    \n",
    "            if len(audio) > 450:\n",
    "                index = (index + 1) % self.__len__()\n",
    "                continue\n",
    "                \n",
    "            break\n",
    "        \n",
    "        return torch.FloatTensor(audio), torch.LongTensor(text)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00a2d5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KSponSpeechDataCollate():\n",
    "    def __call__(self, batch):\n",
    "        audio_lengths = []\n",
    "        text_lengths = []\n",
    "        for audio, text in batch:\n",
    "            audio_lengths.append(len(audio))\n",
    "            text_lengths.append(len(text))\n",
    "            \n",
    "        audio_max_length = max(audio_lengths)\n",
    "        text_max_length = max(text_lengths)\n",
    "        \n",
    "        audio_padded = torch.FloatTensor(len(batch), audio_max_length, 80)\n",
    "        audio_padded.fill_(-5)\n",
    "        audio_lengths = torch.from_numpy(np.array(audio_lengths)).long()\n",
    "        \n",
    "        text_padded = torch.LongTensor(len(batch), text_max_length)\n",
    "        text_padded.zero_()\n",
    "        text_lengths = torch.from_numpy(np.array(text_lengths)).long()\n",
    "        \n",
    "        for i, (audio, text) in enumerate(batch):\n",
    "            audio_padded[i, :len(audio)] = audio\n",
    "            text_padded[i, :len(text)] = text\n",
    "            \n",
    "        outputs = {'audio': audio_padded,\n",
    "                   'audio_lengths': audio_lengths,\n",
    "                   'text': text_padded,\n",
    "                   'text_lengths': text_lengths\n",
    "                  }\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c2d2e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7f5fa3fed820>\n"
     ]
    }
   ],
   "source": [
    "dataset = KSponSpeechDataset(data_dir='dataset')\n",
    "train_loader = DataLoader(dataset, num_workers=8, shuffle=True, batch_size=64, collate_fn=KSponSpeechDataCollate())\n",
    "print(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625c0f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    audio = batch['audio'].data.cpu().numpy()\n",
    "    audio_lengths = batch['audio_lengths'].data.cpu().numpy()\n",
    "    text = batch['text'].data.cpu().numpy()\n",
    "    text_lengths = batch['text_lengths'].data.cpu().numpy()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90c5f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(audio.shape)\n",
    "plt.figure(figsize=[18, 3])\n",
    "librosa.display.specshow(audio[0].T)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a1a301",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(audio_lengths.shape)\n",
    "print(audio_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2b35f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text.shape)\n",
    "print(text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c97a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_lengths.shape)\n",
    "print(text_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f817ec4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
