{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "excited-alloy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hundred-setting",
   "metadata": {},
   "source": [
    "### Bahdanau attention (https://arxiv.org/abs/1409.0473) 을 구현합니다.\n",
    "\n",
    "Attention을하기위한 energy 는다음과같이 계산합니다.\n",
    "\n",
    "$e_{i j}=a\\left(s_{i-1}, h_{j}\\right)=v_{a}^{\\top} \\tanh \\left(W_{a} s_{i-1}+U_{a} h_{j}\\right)$\n",
    "\n",
    "Attention weights는 energy를 softmax연산을 하여 확률분포의 형태를 만들어 구합니다.\n",
    "\n",
    "$\\alpha_{i j}=\\frac{\\exp \\left(e_{i j}\\right)}{\\sum_{k} \\exp \\left(e_{i k}\\right)}$\n",
    "\n",
    "마지막으로 context vector는 energy 와 encoder hidden state들을 weighted sum하여 구합니다.\n",
    "\n",
    "$c_{i}=\\sum_{j} \\alpha_{i j} h_{j}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "subsequent-wednesday",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        batch, enc_len, enc_dim = input_shape[0]\n",
    "        batch, dec_len, dec_dim = input_shape[1]\n",
    "        \n",
    "        # (dec_dim, enc_dim)\n",
    "        self.W = self.add_weight(\"W\", shape=(dec_dim, enc_dim))\n",
    "        # (enc_dim, enc_dim)\n",
    "        self.U = self.add_weight(\"U\",  shape=(enc_dim, enc_dim))\n",
    "        # (enc_dim, 1)\n",
    "        self.v = self.add_weight(\"v\", shape=(enc_dim, 1))\n",
    "      \n",
    "    \n",
    "    def _get_attention_context(self, enc_states, dec_state):\n",
    "        # enc_states : (batch, enc_len, enc_dim)\n",
    "        # dec_state : (batch, dec_dim)\n",
    "        \n",
    "        # (batch, enc_dim)\n",
    "        w = dec_state @ self.W\n",
    "        # (batch, enc_len, enc_dim)\n",
    "        u = enc_states @ self.U\n",
    "        # (batch, enc_len, enc_dim)\n",
    "        t = tf.tanh(tf.expand_dims(w, axis=1) + u)\n",
    "        # (batch, enc_len, 1)\n",
    "        energy = t @ self.v\n",
    "        # (batch, enc_len, 1)\n",
    "        weight = tf.nn.softmax(energy, axis=1)\n",
    "        # (batch, enc_dim)\n",
    "        context = tf.math.reduce_sum(weight * enc_states, axis=1)\n",
    "        \n",
    "        return context\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # (batch, enc_len, enc_dim), (batch, dec_len, dec_dim)\n",
    "        enc_states, dec_states = inputs\n",
    "        batch, enc_len, enc_dim = enc_states.shape\n",
    "        batch, dec_len, dec_dim = dec_states.shape\n",
    "        \n",
    "        contexts = []\n",
    "        for i in range(dec_len):\n",
    "            # (batch, dec_dim)\n",
    "            dec_state = dec_states[:, i]\n",
    "            context = self._get_attention_context(enc_states, dec_state)\n",
    "            contexts.append(context)\n",
    "        \n",
    "        # (batch, dec_len, enc_dim)\n",
    "        contexts = tf.stack(contexts, axis=1)\n",
    "        \n",
    "        return contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "substantial-envelope",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 50, 256)\n"
     ]
    }
   ],
   "source": [
    "batch = 5\n",
    "enc_len = 100\n",
    "dec_len = 50\n",
    "enc_dim = 256\n",
    "dec_dim = 128\n",
    "\n",
    "dec_states = tf.random.normal(shape=(batch, dec_len, dec_dim))\n",
    "enc_states = tf.random.normal(shape=(batch, enc_len, enc_dim))\n",
    "\n",
    "contexts = Attention(name='attention_layer')([enc_states, dec_states])\n",
    "print(contexts.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
