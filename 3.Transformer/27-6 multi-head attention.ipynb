{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "following-pulse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "positive-raise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 스케일드 닷 프로덕트 어텐션 함수\n",
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "  # 어텐션 가중치는 Q와 K의 닷 프로덕트\n",
    "  matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "  # 가중치를 정규화\n",
    "  depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "  logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "  # 패딩에 마스크 추가\n",
    "  if mask is not None:\n",
    "    logits += (mask * -1e9)\n",
    "\n",
    "  # softmax적용\n",
    "  attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "  # 최종 어텐션은 가중치와 V의 닷 프로덕트\n",
    "  output = tf.matmul(attention_weights, value)\n",
    "  return output\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "documentary-silver",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "    super(MultiHeadAttention, self).__init__(name=name)\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "\n",
    "    assert d_model % self.num_heads == 0\n",
    "\n",
    "    self.depth = d_model // self.num_heads\n",
    "\n",
    "    self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "  def split_heads(self, inputs, batch_size):\n",
    "    inputs = tf.reshape(\n",
    "        inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "\n",
    "  def call(self, inputs):\n",
    "    query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
    "        'value'], inputs['mask']\n",
    "    batch_size = tf.shape(query)[0]\n",
    "\n",
    "    # Q, K, V에 각각 Dense를 적용합니다\n",
    "    query = self.query_dense(query)\n",
    "    key = self.key_dense(key)\n",
    "    value = self.value_dense(value)\n",
    "\n",
    "    # 병렬 연산을 위한 머리를 여러 개 만듭니다\n",
    "    query = self.split_heads(query, batch_size)\n",
    "    key = self.split_heads(key, batch_size)\n",
    "    value = self.split_heads(value, batch_size)\n",
    "\n",
    "    # 스케일드 닷 프로덕트 어텐션 함수\n",
    "    scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "    # 어텐션 연산 후에 각 결과를 다시 연결(concatenate)합니다\n",
    "    concat_attention = tf.reshape(scaled_attention,\n",
    "                                  (batch_size, -1, self.d_model))\n",
    "\n",
    "    # 최종 결과에도 Dense를 한 번 더 적용합니다\n",
    "    outputs = self.dense(concat_attention)\n",
    "\n",
    "    return outputs\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "labeled-staff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 0.31194213  0.06846652 -0.14545253 ...  0.34152558 -0.01293674\n",
      "   -0.10417397]\n",
      "  [ 0.06113582 -0.2637791   0.13253844 ...  0.3520121   0.08305789\n",
      "   -0.26265913]\n",
      "  [ 0.05521496 -0.04981989 -0.02933395 ...  0.32582527 -0.00207202\n",
      "    0.01891272]\n",
      "  ...\n",
      "  [ 0.11160279 -0.05528751 -0.08048372 ...  0.2031199  -0.07084238\n",
      "   -0.1108495 ]\n",
      "  [-0.03900976 -0.18911031 -0.11804104 ...  0.20212713 -0.38104144\n",
      "    0.1061062 ]\n",
      "  [ 0.09263898 -0.12962747 -0.06329235 ...  0.02945981 -0.17609507\n",
      "   -0.03076953]]\n",
      "\n",
      " [[-0.14625365 -0.19078861  0.05680215 ...  0.01729532  0.09427337\n",
      "    0.15989985]\n",
      "  [-0.00879875  0.09307674 -0.02225625 ...  0.19250618 -0.1287268\n",
      "   -0.12116873]\n",
      "  [-0.18215227  0.23925833  0.37365118 ...  0.14650804 -0.00389771\n",
      "    0.16848606]\n",
      "  ...\n",
      "  [-0.27207857  0.00303634  0.27001342 ...  0.11705796  0.16962379\n",
      "    0.2031405 ]\n",
      "  [-0.30680832 -0.12186713  0.30924895 ...  0.00931952 -0.22447443\n",
      "    0.06137569]\n",
      "  [ 0.00612278  0.2276799  -0.10122065 ...  0.20439091 -0.08018462\n",
      "    0.20718049]]\n",
      "\n",
      " [[ 0.15926771 -0.03668076 -0.05544756 ...  0.08851743 -0.25373065\n",
      "   -0.15117662]\n",
      "  [-0.03322133 -0.02138285 -0.46764717 ...  0.0602399  -0.3360809\n",
      "    0.19876146]\n",
      "  [-0.06901853  0.36967942 -0.15208626 ... -0.0369864  -0.18323016\n",
      "   -0.04616057]\n",
      "  ...\n",
      "  [ 0.00759642  0.3657835  -0.00347961 ...  0.03156361  0.06318271\n",
      "    0.02990626]\n",
      "  [-0.0878294   0.30483052  0.05971097 ... -0.02088248  0.03554999\n",
      "    0.26272312]\n",
      "  [ 0.02957005  0.34242046 -0.02323282 ... -0.08439431 -0.16231988\n",
      "    0.0844297 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0.10202391  0.26688695 -0.27480865 ...  0.3529317   0.06608267\n",
      "    0.1578736 ]\n",
      "  [ 0.22667995  0.27554655 -0.30997157 ...  0.03043585  0.21118845\n",
      "    0.13907428]\n",
      "  [ 0.27128902  0.30961373 -0.1791372  ...  0.12507349  0.20795801\n",
      "    0.16588014]\n",
      "  ...\n",
      "  [ 0.02407561  0.16581418 -0.02996827 ...  0.12112049 -0.06260885\n",
      "    0.12930545]\n",
      "  [ 0.33161134  0.05869657 -0.10271823 ...  0.08204015  0.0768223\n",
      "    0.08269883]\n",
      "  [ 0.13267432  0.1929251  -0.15302059 ...  0.21575658  0.2042345\n",
      "    0.30988923]]\n",
      "\n",
      " [[ 0.08580911  0.02992464 -0.04731969 ...  0.17658104  0.05030364\n",
      "    0.12651873]\n",
      "  [-0.10515336 -0.03688023  0.10117149 ... -0.04564854  0.07851505\n",
      "   -0.24608657]\n",
      "  [ 0.05638901  0.16922455  0.00179744 ... -0.05067797 -0.00574231\n",
      "    0.11236691]\n",
      "  ...\n",
      "  [-0.01801298  0.29599854  0.17827198 ...  0.10870526 -0.17990662\n",
      "    0.03168255]\n",
      "  [ 0.2831576   0.12467352 -0.05122589 ...  0.18770409 -0.03959902\n",
      "    0.214143  ]\n",
      "  [ 0.04430236 -0.02826503 -0.10537016 ... -0.07136612 -0.11734777\n",
      "    0.16134688]]\n",
      "\n",
      " [[-0.117662   -0.0885231   0.0451721  ...  0.22196285 -0.05932156\n",
      "   -0.06247026]\n",
      "  [ 0.06150096 -0.11576802 -0.03838127 ...  0.03549927  0.06548983\n",
      "   -0.18620065]\n",
      "  [-0.07313629  0.08103658 -0.06622725 ...  0.00546431  0.00876129\n",
      "   -0.05574373]\n",
      "  ...\n",
      "  [ 0.07009701 -0.1204699  -0.1104049  ... -0.09392499 -0.0379571\n",
      "    0.14713076]\n",
      "  [-0.03920211 -0.17372878 -0.06669319 ...  0.10838673 -0.11322954\n",
      "    0.19227603]\n",
      "  [ 0.11366834  0.12930514 -0.02658062 ...  0.02175076 -0.20228162\n",
      "   -0.16842112]]], shape=(16, 100, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "inputs = {'query': tf.random.normal(shape=(16, 100, 512)),\n",
    "          'key': tf.random.normal(shape=(16, 100, 512)),\n",
    "          'value': tf.random.normal(shape=(16, 100, 512)),\n",
    "          'mask': None}\n",
    "outputs = mha(inputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "internal-aircraft",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
